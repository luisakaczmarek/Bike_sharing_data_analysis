{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Luisa Johanna Kaczmarek\n#### Student ID: 16242\n\n#### City chosen for analysis: Chicago\n#### Data range of historical data: January 2023 \u2013 December 2023\n"
      ],
      "metadata": {
        "id": "TnALBLCMtas2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0) Load Data"
      ],
      "metadata": {
        "id": "RE3Ra6TNtas3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "T6bmWunDtas3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qpPFIy93tas4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_trip_data(filepath_pattern):\n",
        "    \"\"\"\n",
        "    Load and concatenate monthly trip files.\n",
        "    Parameters:\n",
        "        filepath_pattern (str): Glob pattern for trip CSV files.\n",
        "    Returns:\n",
        "        pd.DataFrame: All trips concatenated.\n",
        "    \"\"\"\n",
        "    csv_files = sorted(glob.glob(filepath_pattern))\n",
        "    print(f\"Files found: {len(csv_files)}\")\n",
        "\n",
        "    for file in csv_files:\n",
        "        print(f\"File: {file}\")\n",
        "        print(f\"  Size: {os.path.getsize(file):,} bytes\")\n",
        "        print(f\"  Records: {sum(1 for _ in open(file)) - 1:,}\")\n",
        "\n",
        "    dfs = []\n",
        "    for file in csv_files:\n",
        "        df = pd.read_csv(file)\n",
        "        dfs.append(df)\n",
        "\n",
        "    trip_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    for col in ['started_at', 'ended_at']:\n",
        "        if col in trip_data.columns:\n",
        "            trip_data[col] = pd.to_datetime(trip_data[col])\n",
        "\n",
        "    return trip_data"
      ],
      "metadata": {
        "id": "BFABs90Qtas4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trip_data = load_trip_data(\n",
        "    \"/content/drive/MyDrive/emerging_topics_session_5/data/*-divvy-tripdata.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "16yAs7_ntas5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = trip_data.copy()"
      ],
      "metadata": {
        "id": "m7iFeLYptas5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Data Preparation"
      ],
      "metadata": {
        "id": "tFO0k9Cmtas5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count trips by day"
      ],
      "metadata": {
        "id": "Ar7FP6Kptas5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_trips = (\n",
        "    df['started_at']\n",
        "    .dt.floor('D')        # truncate time \u2192 keep date as datetime64\n",
        "    .value_counts()\n",
        "    .sort_index()\n",
        ")"
      ],
      "metadata": {
        "id": "KcNax2g5tas6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_trips"
      ],
      "metadata": {
        "id": "LdllFrKxtas6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for missing dates"
      ],
      "metadata": {
        "id": "jlgG6UcRtas6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify no calendar days are missing from the index\n",
        "full_index = pd.date_range(daily_trips.index.min(), daily_trips.index.max(), freq='D')\n",
        "missing_dates = full_index.difference(daily_trips.index)\n",
        "print(f\"Missing dates: {len(missing_dates)}\")\n",
        "print(daily_trips.isna().sum(), \"NaN values in counts\")"
      ],
      "metadata": {
        "id": "JcNCn6bbtas6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensure index is datetime"
      ],
      "metadata": {
        "id": "0uqdDjxZtas6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(daily_trips.index))\n",
        "print(daily_trips.index.dtype)"
      ],
      "metadata": {
        "id": "VjGDpbgStas6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle timezone"
      ],
      "metadata": {
        "id": "PoRGr789tas6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if timezone-aware\n",
        "daily_trips.index.tz"
      ],
      "metadata": {
        "id": "LAML9IHYtas7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\u2192 Timezone-naive. We know the data is recorded in Chicago local time, so we assign the correct timezone."
      ],
      "metadata": {
        "id": "SHbdyfJ5tas7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_trips.index = daily_trips.index.tz_localize('America/Chicago')\n",
        "print(daily_trips.index.tz)  # verify"
      ],
      "metadata": {
        "id": "Jg-dsvAYtas7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store timeseries as Parquet for reuse"
      ],
      "metadata": {
        "id": "nxRdr2S3tas7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/emerging_topics_session_5/data/\"\n",
        "daily_trips.to_frame('trips').to_parquet(f\"{output_path}daily_trips.parquet\")\n",
        "print(\"Saved daily_trips.parquet\")"
      ],
      "metadata": {
        "id": "G9qfU_4Utas7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Classical Decomposition"
      ],
      "metadata": {
        "id": "DYpKRO6Gtas7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot raw series"
      ],
      "metadata": {
        "id": "iwQOf8Svtas7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(daily_trips.index, daily_trips.values, linewidth=0.8)\n",
        "plt.title('Daily Trip Counts \u2013 Chicago Divvy 2023')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Trip Count')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vK_afWdOtas7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations from raw plot:**\n",
        "- Clear summer peak (Jun\u2013Aug), winter trough (Jan\u2013Feb)\n",
        "- No obvious long-term upward/downward level shift \u2192 levels at year-start \u2248 year-end\n",
        "- High-frequency oscillation visible (weekly pattern)"
      ],
      "metadata": {
        "id": "NO3Sw6A_tas7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decompose"
      ],
      "metadata": {
        "id": "NF20euNctas7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "decomposition = seasonal_decompose(\n",
        "    daily_trips,\n",
        "    model='additive',         # additive: Y = Trend + Seasonal + Residual\n",
        "    period=7,                 # weekly seasonality\n",
        "    extrapolate_trend='freq'  # handles NaN at edges\n",
        ")\n",
        "\n",
        "trend    = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid\n",
        "observed = decomposition.observed"
      ],
      "metadata": {
        "id": "fHfJCygFtas8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
        "\n",
        "decomposition.observed.plot(ax=axes[0], color='black', linewidth=0.8)\n",
        "axes[0].set_title('Original Time Series', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Trip Count')\n",
        "axes[0].grid(True, color='lightgrey', linewidth=0.5)\n",
        "axes[0].set_xlabel('')\n",
        "\n",
        "decomposition.trend.plot(ax=axes[1], color='royalblue', linewidth=1.2)\n",
        "axes[1].set_title('Trend Component', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Trend')\n",
        "axes[1].grid(True, color='lightgrey', linewidth=0.5)\n",
        "axes[1].set_xlabel('')\n",
        "\n",
        "decomposition.seasonal.plot(ax=axes[2], color='seagreen', linewidth=0.8)\n",
        "axes[2].set_title('Seasonal Component (Weekly Pattern, period=7)', fontsize=12, fontweight='bold')\n",
        "axes[2].set_ylabel('Seasonal')\n",
        "axes[2].grid(True, color='lightgrey', linewidth=0.5)\n",
        "axes[2].set_xlabel('')\n",
        "\n",
        "decomposition.resid.plot(ax=axes[3], color='crimson', linewidth=0.8)\n",
        "axes[3].axhline(y=0, color='grey', linestyle='--', linewidth=0.8, alpha=0.7)\n",
        "axes[3].set_title('Residual Component', fontsize=12, fontweight='bold')\n",
        "axes[3].set_ylabel('Residual')\n",
        "axes[3].set_xlabel('Date')\n",
        "axes[3].grid(True, color='lightgrey', linewidth=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oKMeCJTRtas8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis:**\n",
        "\n",
        "**Trend**\n",
        "- Strong upward trend Jan \u2192 late summer; peak around Jul\u2013Aug\n",
        "- Clear decline Sep \u2192 Dec\n",
        "- Likely driven by weather, daylight hours, and seasonal leisure demand\n",
        "- No permanent level shift \u2192 seasonality dominates over a long-run trend\n",
        "\n",
        "**Seasonal (weekly)**\n",
        "- Very regular, high-amplitude weekly cycle, stable across all 12 months\n",
        "- Amplitude \u2248 \u00b11,000 trips, large relative to residual noise\n",
        "- Confirms weekly seasonality is a dominant structural feature\n",
        "\n",
        "**Residual**\n",
        "- Centered around zero \u2192 decomposition fits well\n",
        "- Variance visibly higher in summer months (heteroskedasticity)\n",
        "- Several sharp spikes (positive & negative) \u2192 potential outliers from events or weather\n",
        "- Mild heteroskedasticity suggests a variance-stabilising transform may improve downstream modelling\n",
        "\n",
        "**Model implications**\n",
        "- Additive decomposition is appropriate (constant seasonal amplitude)\n",
        "- Heteroskedasticity \u2192 consider log or Box-Cox transform before ARIMA fitting\n",
        "- Weekly seasonality should be explicitly modelled (e.g., SARIMA with s=7)"
      ],
      "metadata": {
        "id": "Dc0x762Xtas8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formal test: Is a variance-stabilising transform needed? (Box\u2013Cox)"
      ],
      "metadata": {
        "id": "dHqiiA4Otas8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import boxcox_normmax, boxcox\n",
        "\n",
        "# 1) Build trend-and-seasonally-adjusted series for a clean variance assessment\n",
        "#    Using the decomposition residual directly is the cleanest approach:\n",
        "#    residual = observed \u2212 trend \u2212 seasonal  (all systematic variation removed)\n",
        "y_resid = decomposition.resid.dropna()\n",
        "\n",
        "# 2) Box\u2013Cox requires strictly positive values \u2192 shift if needed\n",
        "shift = 0.0\n",
        "if y_resid.min() <= 0:\n",
        "    shift = 1.0 - y_resid.min()\n",
        "y_pos = y_resid + shift\n",
        "\n",
        "# 3) Estimate lambda (MLE)\n",
        "lambda_bc = boxcox_normmax(y_pos.values, method='mle')\n",
        "print(f\"Box\u2013Cox lambda (on decomposition residuals): {lambda_bc:.4f}\")\n",
        "if shift != 0:\n",
        "    print(f\"Applied shift to make series positive: +{shift:.4f}\")\n",
        "\n",
        "# 4) (Optional) apply same lambda to the ORIGINAL series for downstream modelling\n",
        "y_orig_pos = daily_trips.astype(float)\n",
        "if y_orig_pos.min() <= 0:\n",
        "    y_orig_pos = y_orig_pos + (1.0 - y_orig_pos.min())\n",
        "y_bc = pd.Series(\n",
        "    boxcox(y_orig_pos.values, lmbda=lambda_bc),\n",
        "    index=daily_trips.index,\n",
        "    name='daily_trips_boxcox'\n",
        ")\n",
        "y_bc.head()"
      ],
      "metadata": {
        "id": "v91pWxWktas8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of Box\u2013Cox lambda:**\n",
        "\n",
        "- \u03bb \u2260 1 \u2192 variance is not stable \u2192 a transformation is useful before modelling\n",
        "- \u03bb > 0 \u2192 power-type transformation (not log)\n",
        "- \u03bb \u2248 0.5 \u2192 very close to a **square-root transformation** (\u221ay)\n",
        "\n",
        "**Implication for modelling:**\n",
        "- Use the exact Box\u2013Cox with the estimated \u03bb, **or**\n",
        "- Use \u221ay as a simpler approximation with easier interpretability"
      ],
      "metadata": {
        "id": "iGgsfC4gtas8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantify explained variance"
      ],
      "metadata": {
        "id": "wFHQUjretas8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "resid_var = decomposition.resid.var()\n",
        "resid_sd  = np.sqrt(resid_var)\n",
        "\n",
        "orig_var  = daily_trips.var()\n",
        "orig_sd   = np.sqrt(orig_var)\n",
        "orig_mean = daily_trips.mean()\n",
        "\n",
        "print(f\"Original variance:         {orig_var:>12,.2f}\")\n",
        "print(f\"Original SD:               {orig_sd:>12,.2f}\")\n",
        "print(f\"Original mean:             {orig_mean:>12,.2f}\")\n",
        "print()\n",
        "print(f\"Residual variance:         {resid_var:>12,.2f}\")\n",
        "print(f\"Residual SD:               {resid_sd:>12,.2f}\")\n",
        "print()\n",
        "print(f\"Residual SD / Original SD: {resid_sd / orig_sd:.2f}\")\n",
        "print(f\"Explained variance (R\u00b2):   {1 - resid_var/orig_var:.2%}\")\n",
        "print(f\"Residual SD / Mean:        {resid_sd / orig_mean:.2f}\")"
      ],
      "metadata": {
        "id": "mifgY6BHtas8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of Residual Variance:**\n",
        "\n",
        "- Residual SD / Original SD \u2248 **0.38** \u2192 decomposition removed most variability\n",
        "- Explained variance \u2248 **86%** \u2192 trend + weekly seasonality capture the dominant structure\n",
        "- Residual SD \u2248 **3,100 trips** \u2192 typical unexplained fluctuation per day\n",
        "- Residual SD / Mean \u2248 **0.20** \u2192 residual noise is about 20% of the average level\n",
        "- \u03bb \u2248 **0.55** \u2192 mild power transformation (\u2248 square-root) may further stabilise variance before ARIMA"
      ],
      "metadata": {
        "id": "qBFytL6Etas8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Multiple Seasonality Analysis"
      ],
      "metadata": {
        "id": "9kyVR5ROtas9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weekly pattern"
      ],
      "metadata": {
        "id": "J6CN6fdKtas9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "day_order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
        "weekday_avg = daily_trips.groupby(daily_trips.index.day_name()).mean().reindex(day_order)\n",
        "\n",
        "print(\"Average trips by day of week:\")\n",
        "print(weekday_avg.round(0).to_string())\n",
        "print(f\"\\nBusiest day:  {weekday_avg.idxmax()} ({weekday_avg.max():.0f} trips)\")\n",
        "print(f\"Quietest day: {weekday_avg.idxmin()} ({weekday_avg.min():.0f} trips)\")"
      ],
      "metadata": {
        "id": "DwEifD3Jtas9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight on weekly pattern:**\n",
        "\n",
        "- Saturday is the **busiest day**, suggesting leisure-driven demand peaks on weekends\n",
        "- Monday and Sunday show the **lowest** ridership \u2014 Monday may reflect trip-planning lag after the weekend; Sunday's lower level vs Saturday is a consistent asymmetry in bike-share systems (shorter activity windows, more people resting)\n",
        "- Tue\u2013Fri cluster closely together, indicating stable weekday commuter demand\n",
        "- The Saturday spike should be treated as a **leisure** effect, structurally distinct from the weekday commuter pattern"
      ],
      "metadata": {
        "id": "LpqPF8BWtas9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monthly pattern"
      ],
      "metadata": {
        "id": "a4_knmMltas-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "month_names = ['January','February','March','April','May','June',\n",
        "               'July','August','September','October','November','December']\n",
        "\n",
        "monthly_avg = daily_trips.groupby(daily_trips.index.month).mean()\n",
        "monthly_avg.index = month_names\n",
        "\n",
        "print(\"Average daily trips by month:\")\n",
        "print(monthly_avg.round(0).to_string())\n",
        "print(f\"\\nBusiest month:  {monthly_avg.idxmax()} ({monthly_avg.max():.0f} trips)\")\n",
        "print(f\"Quietest month: {monthly_avg.idxmin()} ({monthly_avg.min():.0f} trips)\")"
      ],
      "metadata": {
        "id": "7erySnBGtas-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualise both seasonality patterns"
      ],
      "metadata": {
        "id": "a51sWMfxtas-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y = daily_trips.astype(float)\n",
        "\n",
        "dow_avg = y.groupby(y.index.dayofweek).mean()   # 0=Mon \u2026 6=Sun\n",
        "dow_names = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
        "\n",
        "m_avg = y.groupby(y.index.month).mean()\n",
        "m_names = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
        "m_avg.index = [m_names[m-1] for m in m_avg.index]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5), dpi=150)\n",
        "\n",
        "# \u2500\u2500 Day-of-week \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "x = np.arange(len(dow_avg))\n",
        "bars1 = axes[0].bar(x, dow_avg.values, color='#7fb3d5')\n",
        "axes[0].set_title('Day-of-Week Seasonality Pattern', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Average Daily Trips')\n",
        "axes[0].set_xlabel('Day of Week')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(dow_names, rotation=30, ha='right')\n",
        "axes[0].grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "# Highlight highest (green) and two lowest (red)\n",
        "lowest_two  = set(dow_avg.nsmallest(2).index)\n",
        "highest_one = dow_avg.idxmax()\n",
        "for i, b in enumerate(bars1):\n",
        "    if i == highest_one:\n",
        "        b.set_color('#7FBF7F')\n",
        "    elif i in lowest_two:\n",
        "        b.set_color('#F4A6A6')\n",
        "for b in bars1:\n",
        "    axes[0].text(b.get_x() + b.get_width()/2, b.get_height() + 50,\n",
        "                 f\"{b.get_height():.0f}\", ha='center', va='bottom', fontsize=7)\n",
        "\n",
        "# \u2500\u2500 Month-of-year \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "x2 = np.arange(len(m_avg))\n",
        "bars2 = axes[1].bar(x2, m_avg.values, color='#7fb3d5')\n",
        "axes[1].set_title('Month-of-Year Seasonality Pattern', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Average Daily Trips')\n",
        "axes[1].set_xlabel('Month')\n",
        "axes[1].set_xticks(x2)\n",
        "axes[1].set_xticklabels(m_avg.index, rotation=30, ha='right')\n",
        "axes[1].grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "max_i = int(np.argmax(m_avg.values))\n",
        "min_i = int(np.argmin(m_avg.values))\n",
        "bars2[max_i].set_color('#7FBF7F')   # highlight peak month\n",
        "bars2[min_i].set_color('#F4A6A6')   # highlight trough month\n",
        "for b in bars2:\n",
        "    axes[1].text(b.get_x() + b.get_width()/2, b.get_height() + 100,\n",
        "                 f\"{b.get_height():.0f}\", ha='center', va='bottom', fontsize=7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iM8u97T0tas-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights**\n",
        "\n",
        "**Day-of-Week Seasonality**\n",
        "- Saturday \u2248 **16,992** \u2192 highest demand (leisure peak)\n",
        "- Monday \u2248 **14,027** and Sunday \u2248 **14,049** \u2192 lowest demand days\n",
        "- Weekdays (Tue\u2013Fri) tightly clustered \u2248 15,800\u201316,500 \u2192 stable commuter baseline\n",
        "- Weekend effect is **asymmetric** \u2192 strong Saturday spike, subdued Sunday; likely due to different leisure rhythms (Saturday outings vs Sunday rest)\n",
        "\n",
        "**Month-of-Year Seasonality**\n",
        "- August \u2248 **24,893** \u2192 annual peak (strong summer demand)\n",
        "- July \u2248 **24,763** and June \u2248 **23,987** \u2192 sustained summer high season\n",
        "- January \u2248 **6,139** \u2192 lowest month (harsh Chicago winter)\n",
        "- Seasonal amplitude is large: peak month \u2248 **4\u00d7 January level**\n",
        "- Strong ramp from Jan \u2192 Aug, then steady decline to Dec"
      ],
      "metadata": {
        "id": "8tgjRaCDtas-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Researching Anomalies"
      ],
      "metadata": {
        "id": "ypoUZsOstas-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to identify and explain anomalies, I follow a two-step process:\n",
        "1. **Identify outliers** using the decomposition residual (trend + weekly seasonality already removed)\n",
        "2. **Explain outliers** by overlaying known external events (weather, city events, holidays)"
      ],
      "metadata": {
        "id": "twXUL05Mtas_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Identify outliers using decomposition residuals"
      ],
      "metadata": {
        "id": "5jcll1xotas_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Methodological note:** Earlier approaches subtracted only the weekly seasonal component (`daily_trips - decomposition.seasonal`), leaving the large annual trend in the series. This causes IQR and Z-score methods to find zero outliers, because the trend inflates the spread of the distribution.\n",
        "\n",
        "The correct approach is to use `decomposition.resid` directly, which has **both** trend and seasonality removed, leaving only the genuinely unexplained day-to-day variation."
      ],
      "metadata": {
        "id": "C9qi5FlQtas_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Use the decomposition residual: trend + seasonality already removed\n",
        "resid = decomposition.resid.dropna()\n",
        "\n",
        "# \u2500\u2500 IQR method (standard 1.5\u00d7 fence) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "Q1 = resid.quantile(0.25)\n",
        "Q3 = resid.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_iqr = Q1 - 1.5 * IQR\n",
        "upper_iqr = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers_iqr = resid[(resid < lower_iqr) | (resid > upper_iqr)]\n",
        "print(f\"IQR outliers (1.5\u00d7): {len(outliers_iqr)}\")\n",
        "print(outliers_iqr.sort_values())"
      ],
      "metadata": {
        "id": "b_qkc-wqtas_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \u2500\u2500 Z-score method (|z| > 2.5) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "z_scores = (resid - resid.mean()) / resid.std()\n",
        "outliers_z = resid[np.abs(z_scores) > 2.5]\n",
        "\n",
        "print(f\"Z-score outliers (|z| > 2.5): {len(outliers_z)}\")\n",
        "print(outliers_z.sort_values())"
      ],
      "metadata": {
        "id": "jm6TajtNtas_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \u2500\u2500 Combine outlier candidates from both methods \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "all_outlier_dates = outliers_iqr.index.union(outliers_z.index)\n",
        "\n",
        "outlier_df = pd.DataFrame({\n",
        "    'residual':   resid.loc[all_outlier_dates],\n",
        "    'z_score':    z_scores.loc[all_outlier_dates],\n",
        "    'raw_trips':  daily_trips.loc[all_outlier_dates]\n",
        "}).sort_values('residual')\n",
        "\n",
        "print(f\"Total unique outlier candidates: {len(outlier_df)}\")\n",
        "print(outlier_df.round(1))"
      ],
      "metadata": {
        "id": "zt8ttt9Otas_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Visualise residuals with outliers flagged"
      ],
      "metadata": {
        "id": "DDvj6oxZtas_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(14, 4), dpi=150)\n",
        "\n",
        "ax.plot(resid.index, resid.values, color='#7fb3d5', linewidth=0.8, label='Residual')\n",
        "ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
        "ax.axhline(upper_iqr, color='orange', linestyle=':', linewidth=1, label=f'IQR fence (\u00b1{upper_iqr:.0f})')\n",
        "ax.axhline(lower_iqr, color='orange', linestyle=':', linewidth=1)\n",
        "\n",
        "# Mark outliers\n",
        "if len(outlier_df) > 0:\n",
        "    ax.scatter(outlier_df.index, outlier_df['residual'],\n",
        "               color='crimson', zorder=5, s=40, label='Outlier candidate')\n",
        "    for dt, row in outlier_df.iterrows():\n",
        "        ax.annotate(dt.strftime('%b %d'), xy=(dt, row['residual']),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=7, color='crimson')\n",
        "\n",
        "ax.set_title('Decomposition Residuals with Outlier Candidates', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Residual (trips)')\n",
        "ax.set_xlabel('Date')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3wuATtE0tas_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Contextual anomaly \u2014 day-of-week residuals"
      ],
      "metadata": {
        "id": "MK7teqpbtatA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \u2500\u2500 Detect contextual anomalies: does a given day deviate from its day-of-week average?\n",
        "dow_mean = daily_trips.groupby(daily_trips.index.dayofweek).mean()\n",
        "dow_std  = daily_trips.groupby(daily_trips.index.dayofweek).std()\n",
        "\n",
        "contextual_z = (daily_trips - daily_trips.index.dayofweek.map(dow_mean)) /                daily_trips.index.dayofweek.map(dow_std)\n",
        "\n",
        "contextual_outliers = daily_trips[np.abs(contextual_z) > 2.5].copy().to_frame('trips')\n",
        "contextual_outliers['day_name']     = contextual_outliers.index.day_name()\n",
        "contextual_outliers['contextual_z'] = contextual_z.loc[contextual_outliers.index]\n",
        "\n",
        "print(f\"Contextual anomalies (|z|>2.5 within each weekday group): {len(contextual_outliers)}\")\n",
        "print(contextual_outliers.sort_values('contextual_z').round(2))"
      ],
      "metadata": {
        "id": "ioq19tOntatA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: External event overlay & anomaly explanations"
      ],
      "metadata": {
        "id": "qRV7I5xdtatA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key anomaly explanations for Chicago Divvy 2023:**\n",
        "\n",
        "**Negative anomalies (unexpected ridership drops):**\n",
        "\n",
        "| Date(s) | Type | Explanation |\n",
        "|---|---|---|\n",
        "| Late Jan / early Feb 2023 | Weather | Chicago experienced a **polar vortex event** with temperatures dropping to \u221215\u00b0C to \u221220\u00b0C, well below seasonal norms. Cycling becomes dangerous and impractical at these temperatures, suppressing ridership far below typical winter levels. |\n",
        "| Early April 2023 | Weather | A late-season snowstorm hit Chicago on April 4\u20135, 2023, with several inches of snow. Spring riders who had returned were forced off bikes, causing a sharp dip. |\n",
        "| Day-after major holidays (e.g., Jan 2, July 5, Nov 24) | Holiday lag | The day *after* a public holiday tends to see a rebound or continuation of suppressed commuter demand. Sunday-level demand on unusual weekdays creates contextual anomalies. |\n",
        "\n",
        "**Positive anomalies (unexpected ridership spikes):**\n",
        "\n",
        "| Date(s) | Type | Explanation |\n",
        "|---|---|---|\n",
        "| Aug 3\u20136, 2023 | Event | **Lollapalooza** takes place annually in Grant Park. The festival draws 400,000+ visitors over four days and is Chicago's largest annual event. Bike-share to/from Grant Park and nearby stations sees a substantial spike. |\n",
        "| May 27\u201329, 2023 | Event | **Memorial Day weekend** \u2014 a major leisure cycling weekend in Chicago, with good spring weather. Higher-than-expected Saturday/Sunday trips. |\n",
        "| June 2023 (Pride) | Event | **Chicago Pride Fest and Pride Parade** (late June) attract large crowds to the Northside, typically boosting nearby station ridership. |\n",
        "| Sep 2023 | Weather | An unusually warm September (Indian Summer) in 2023 extended the high-ridership summer period, potentially causing above-trend residuals early in the month. |\n",
        "\n",
        "**Structural note \u2014 Divvy e-bike fleet expansion (2023):**\n",
        "Divvy significantly expanded its e-bike fleet in 2023. This could explain an above-trend residual in spring 2023 as new supply stimulated demand beyond what seasonality alone would predict. This is a **supply-side anomaly** that global outlier methods would not isolate cleanly from the seasonal ramp.\n",
        "\n",
        "---\n",
        "\n",
        "**Interpretation summary:**\n",
        "- No statistical outliers were found when using only weekly seasonal adjustment (the trend was still present, masking spikes)\n",
        "- Using `decomposition.resid` correctly removes both trend and seasonality, revealing the genuine day-level deviations\n",
        "- The dominant drivers of anomalies in Chicago Divvy 2023 are: **extreme cold weather, major events (Lollapalooza, Pride, Memorial Day), and holiday effects**\n",
        "- Demand variability is largely **structurally driven** (trend + seasonality), confirming that SARIMA with explicit seasonal components is the appropriate next modelling step"
      ],
      "metadata": {
        "id": "0nFb-lN3tatA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary & modelling implications\n**Key findings:**\n\n1. **Data:** 12 months of Chicago Divvy 2023 data loaded and aggregated to daily trip counts (5,519,877 total trips across 365 days)\n2. **Decomposition:** Additive model with period=7 explains \u2248 86% of total variance; residuals are centred around zero\n3. **Heteroskedasticity:** Residual variance increases in summer \u2192 Box-Cox \u03bb \u2248 0.55 (\u2248 \u221ay) recommended before ARIMA fitting\n4. **Weekly seasonality:** Saturday = peak (leisure); Mon/Sun = trough; Tue\u2013Fri stable commuter baseline\n5. **Annual seasonality:** August peak \u2248 4\u00d7 January trough; strong Chicago winter suppression\n6. **Anomalies:** Driven by polar vortex cold snaps, major events (Lollapalooza, Pride), and holiday effects\n7. **Modelling recommendation:** SARIMA(p,d,q)(P,D,Q)[7] with possible holiday dummies; apply Box-Cox before fitting"
      ],
      "metadata": {
        "id": "4Sq3q_zMtatA"
      }
    }
  ]
}