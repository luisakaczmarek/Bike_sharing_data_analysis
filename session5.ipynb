{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd5bdff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# # Bike Share Project — Data Acquisition, Quality Assessment, Initial Visualization\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Fill in:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Bike Share Project — Data Acquisition, Quality Assessment, Initial Visualization\n",
    "# Fill in:\n",
    "# - Name, Student ID\n",
    "# - City chosen\n",
    "# - Historical year (2022 or 2023)\n",
    "# - Portal/source link\n",
    "# - Download confirmation (filenames, sizes, missing months)\n",
    "#\n",
    "# This notebook assumes:\n",
    "# - Current station status saved by `scrape_citybikes.py` in ./data/\n",
    "# - Historical trip CSVs saved locally in ./data/historical/ (one or many files)\n",
    "\n",
    "# %%\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "HIST_DIR = DATA_DIR / \"historical\"\n",
    "HIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Load current station status (from the scraper output)\n",
    "\n",
    "# %%\n",
    "# Pick the latest station_status file\n",
    "station_files = sorted(DATA_DIR.glob(\"station_status_*.parquet\")) + sorted(DATA_DIR.glob(\"station_status_*.csv\"))\n",
    "if not station_files:\n",
    "    raise FileNotFoundError(\"No station_status files found in ./data/. Run scrape_citybikes.py first.\")\n",
    "\n",
    "latest_station_file = station_files[-1]\n",
    "print(\"Using:\", latest_station_file.name)\n",
    "\n",
    "if latest_station_file.suffix == \".parquet\":\n",
    "    station_status = pd.read_parquet(latest_station_file)\n",
    "else:\n",
    "    station_status = pd.read_csv(latest_station_file)\n",
    "\n",
    "station_status[\"timestamp\"] = pd.to_datetime(station_status[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "display(station_status.head())\n",
    "station_status.info()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Task 1.2 — Load historical trip data (you download it from your city's portal)\n",
    "# Put all CSVs for one full year in: `./data/historical/`\n",
    "# Expected columns (names vary by city): trip_id, start_time, end_time, start_station_id, end_station_id, duration\n",
    "\n",
    "# %%\n",
    "trip_files = sorted(HIST_DIR.glob(\"*.csv\"))\n",
    "if not trip_files:\n",
    "    raise FileNotFoundError(\"No historical trip CSVs found in ./data/historical/. Add your downloaded files first.\")\n",
    "\n",
    "print(\"Files found:\")\n",
    "for f in trip_files:\n",
    "    print(\" -\", f.name)\n",
    "\n",
    "# Load and concatenate\n",
    "trips_raw = pd.concat((pd.read_csv(f) for f in trip_files), ignore_index=True)\n",
    "display(trips_raw.head())\n",
    "trips_raw.info()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Map your city's column names to standard names\n",
    "# Edit the mapping below so the right columns are used.\n",
    "\n",
    "# %%\n",
    "# EDIT THIS MAPPING to match your dataset columns\n",
    "COLMAP = {\n",
    "    \"trip_id\": \"trip_id\",\n",
    "    \"start_time\": \"start_time\",\n",
    "    \"end_time\": \"end_time\",\n",
    "    \"start_station_id\": \"start_station_id\",\n",
    "    \"end_station_id\": \"end_station_id\",\n",
    "    \"duration\": \"duration\",  # seconds or minutes depending on city\n",
    "}\n",
    "\n",
    "missing = [v for v in COLMAP.values() if v not in trips_raw.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Update COLMAP. Missing columns in trips_raw: {missing}\")\n",
    "\n",
    "trips = trips_raw[list(COLMAP.values())].rename(columns={v: k for k, v in COLMAP.items()}).copy()\n",
    "\n",
    "# Parse timestamps\n",
    "trips[\"start_time\"] = pd.to_datetime(trips[\"start_time\"], errors=\"coerce\", utc=True)\n",
    "trips[\"end_time\"] = pd.to_datetime(trips[\"end_time\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Ensure duration numeric\n",
    "trips[\"duration\"] = pd.to_numeric(trips[\"duration\"], errors=\"coerce\")\n",
    "\n",
    "display(trips.head())\n",
    "trips.info()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Task 2.1 — Missing value analysis\n",
    "\n",
    "# %%\n",
    "missing_count = trips.isna().sum()\n",
    "missing_pct = (missing_count / len(trips) * 100).round(3)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    \"column\": missing_count.index,\n",
    "    \"missing_count\": missing_count.values,\n",
    "    \"missing_pct\": missing_pct.values,\n",
    "})\n",
    "\n",
    "# Critical fields: timestamps + station IDs\n",
    "critical_cols = [\"start_time\", \"end_time\", \"start_station_id\", \"end_station_id\"]\n",
    "rows_missing_critical = trips[critical_cols].isna().any(axis=1).sum()\n",
    "\n",
    "print(\"Rows with any missing critical field:\", rows_missing_critical)\n",
    "\n",
    "display(missing_summary.sort_values(\"missing_pct\", ascending=False))\n",
    "\n",
    "# Optional pattern check by month (systematic missingness)\n",
    "trips[\"_month\"] = trips[\"start_time\"].dt.to_period(\"M\")\n",
    "pattern = (trips[critical_cols].isna().any(axis=1)\n",
    "           .groupby(trips[\"_month\"])\n",
    "           .mean()\n",
    "           .mul(100)\n",
    "           .rename(\"pct_rows_missing_critical\")\n",
    "           .reset_index())\n",
    "display(pattern)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Task 2.2 — Outlier detection\n",
    "# Duration rules:\n",
    "# - < 60 seconds\n",
    "# - > 24 hours (86400 seconds)\n",
    "# If your duration is in minutes, adjust thresholds accordingly.\n",
    "\n",
    "# %%\n",
    "# If your duration is in minutes, set DURATION_UNIT=\"minutes\" and adjust thresholds.\n",
    "DURATION_UNIT = \"seconds\"  # \"seconds\" or \"minutes\"\n",
    "\n",
    "if DURATION_UNIT == \"seconds\":\n",
    "    low_thr = 60\n",
    "    high_thr = 24 * 3600\n",
    "else:\n",
    "    low_thr = 1\n",
    "    high_thr = 24 * 60\n",
    "\n",
    "duration = trips[\"duration\"]\n",
    "\n",
    "pctiles = duration.quantile([0.01, 0.05, 0.95, 0.99]).to_frame(\"duration\").T\n",
    "display(pctiles)\n",
    "\n",
    "too_short = duration < low_thr\n",
    "too_long = duration > high_thr\n",
    "\n",
    "print(\"Too short count:\", int(too_short.sum()))\n",
    "print(\"Too long count:\", int(too_long.sum()))\n",
    "\n",
    "# Temporal anomalies\n",
    "now_utc = pd.Timestamp.utcnow().tz_localize(\"UTC\")\n",
    "future_start = trips[\"start_time\"] > now_utc\n",
    "future_end = trips[\"end_time\"] > now_utc\n",
    "\n",
    "print(\"Future start_time count:\", int(future_start.sum()))\n",
    "print(\"Future end_time count:\", int(future_end.sum()))\n",
    "\n",
    "dup_trip_id = trips[\"trip_id\"].duplicated(keep=False)\n",
    "print(\"Duplicate trip_id count:\", int(dup_trip_id.sum()))\n",
    "\n",
    "# Spatial outliers (only possible if your trips include station coordinates; many trip datasets don't)\n",
    "# If you have station lat/lon in the trip file, add checks here.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Cleaning (create `trips_clean`) + Task 2.3 log\n",
    "# Make dataset-specific choices here (drop, impute, flag).\n",
    "# Below is a common baseline: drop missing critical fields, remove impossible durations, drop duplicate trip_id.\n",
    "\n",
    "# %%\n",
    "before_rows = len(trips)\n",
    "\n",
    "trips_clean = trips.copy()\n",
    "\n",
    "# Drop rows missing critical fields\n",
    "trips_clean = trips_clean.dropna(subset=critical_cols)\n",
    "\n",
    "# Remove impossible durations\n",
    "trips_clean = trips_clean[~too_short & ~too_long]\n",
    "\n",
    "# Remove duplicate trip_id (keep first)\n",
    "trips_clean = trips_clean.drop_duplicates(subset=[\"trip_id\"], keep=\"first\")\n",
    "\n",
    "after_rows = len(trips_clean)\n",
    "\n",
    "print(\"Rows before:\", before_rows)\n",
    "print(\"Rows after :\", after_rows)\n",
    "print(\"Removed    :\", before_rows - after_rows, f\"({(before_rows - after_rows)/before_rows*100:.2f}%)\")\n",
    "\n",
    "# Save cleaned data\n",
    "OUT_TRIPS_CLEAN = DATA_DIR / \"trips_clean.parquet\"\n",
    "trips_clean.to_parquet(OUT_TRIPS_CLEAN, index=False)\n",
    "print(\"Saved:\", OUT_TRIPS_CLEAN)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Task 2.3 — Cleaning decision log (write 250–400 words)\n",
    "# Replace this text with your dataset-specific log:\n",
    "#\n",
    "# - Which outliers you removed and why\n",
    "# - Which outliers you kept and why\n",
    "# - How you handled missing values\n",
    "# - Impact on dataset size\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Task 3.1 — Temporal overview plots\n",
    "# 1) Daily trip counts\n",
    "# 2) Hourly patterns: weekday vs weekend (average trips per hour)\n",
    "# 3) Monthly trend + moving average preview\n",
    "\n",
    "# %%\n",
    "tc = trips_clean.copy()\n",
    "tc[\"date\"] = tc[\"start_time\"].dt.date\n",
    "tc[\"hour\"] = tc[\"start_time\"].dt.hour\n",
    "tc[\"weekday\"] = tc[\"start_time\"].dt.weekday  # 0=Mon\n",
    "tc[\"is_weekend\"] = tc[\"weekday\"].isin([5, 6])\n",
    "\n",
    "# 1) Daily\n",
    "daily = tc.groupby(\"date\").size().rename(\"trips\").reset_index()\n",
    "plt.figure()\n",
    "plt.plot(pd.to_datetime(daily[\"date\"]), daily[\"trips\"])\n",
    "plt.title(\"Daily trip counts\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# 2) Hourly weekday vs weekend\n",
    "hourly = tc.groupby([\"is_weekend\", \"hour\"]).size().rename(\"trips\").reset_index()\n",
    "hourly_avg = hourly.copy()\n",
    "# average trips per hour across all days -> compute by (day,hour) then average\n",
    "tc[\"day\"] = tc[\"start_time\"].dt.date\n",
    "day_hour = tc.groupby([\"is_weekend\", \"day\", \"hour\"]).size().rename(\"trips\").reset_index()\n",
    "hour_profile = day_hour.groupby([\"is_weekend\", \"hour\"])[\"trips\"].mean().reset_index()\n",
    "\n",
    "plt.figure()\n",
    "for flag, label in [(False, \"Weekday\"), (True, \"Weekend\")]:\n",
    "    subset = hour_profile[hour_profile[\"is_weekend\"] == flag]\n",
    "    plt.plot(subset[\"hour\"], subset[\"trips\"], label=label)\n",
    "plt.title(\"Average trips by hour: weekday vs weekend\")\n",
    "plt.xlabel(\"Hour of day\")\n",
    "plt.ylabel(\"Average trips\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# 3) Monthly + moving average (use daily series for MA)\n",
    "daily_ts = daily.copy()\n",
    "daily_ts[\"date\"] = pd.to_datetime(daily_ts[\"date\"])\n",
    "daily_ts = daily_ts.sort_values(\"date\")\n",
    "daily_ts[\"ma_7\"] = daily_ts[\"trips\"].rolling(7).mean()\n",
    "\n",
    "monthly = tc.groupby(tc[\"start_time\"].dt.to_period(\"M\")).size().rename(\"trips\").reset_index()\n",
    "monthly[\"start_time\"] = monthly[\"start_time\"].dt.to_timestamp()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(daily_ts[\"date\"], daily_ts[\"trips\"], label=\"Daily\")\n",
    "plt.plot(daily_ts[\"date\"], daily_ts[\"ma_7\"], label=\"7-day MA\")\n",
    "plt.title(\"Daily trips with moving average\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(monthly[\"start_time\"], monthly[\"trips\"])\n",
    "plt.title(\"Monthly trip counts\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Task 3.2 — Spatial overview\n",
    "# Requires station coordinates + station usage (trips originated).\n",
    "# We compute origins per start_station_id and join to station_status lat/lon.\n",
    "\n",
    "# %%\n",
    "# Aggregate usage by start station\n",
    "origin_counts = tc.groupby(\"start_station_id\").size().rename(\"origin_trips\").reset_index()\n",
    "\n",
    "# Join with current station metadata (id formats may differ across datasets)\n",
    "stations_map = station_status.rename(columns={\"station_id\": \"start_station_id\"})\n",
    "spatial = origin_counts.merge(stations_map[[\"start_station_id\", \"name\", \"latitude\", \"longitude\"]], on=\"start_station_id\", how=\"left\")\n",
    "\n",
    "# Usage quartiles (only for non-null counts)\n",
    "spatial[\"usage_quartile\"] = pd.qcut(spatial[\"origin_trips\"].rank(method=\"first\"), 4, labels=[\"low\", \"mid-low\", \"mid-high\", \"high\"])\n",
    "\n",
    "display(spatial.head())\n",
    "print(\"Stations missing coordinates after join:\", spatial[\"latitude\"].isna().sum())\n",
    "\n",
    "# %%\n",
    "# Map with folium (interactive)\n",
    "import folium\n",
    "\n",
    "# Center map on median coordinates (drop NA)\n",
    "center_lat = spatial[\"latitude\"].dropna().median()\n",
    "center_lon = spatial[\"longitude\"].dropna().median()\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "# scale marker radius\n",
    "max_trips = spatial[\"origin_trips\"].max()\n",
    "def radius(x):\n",
    "    if pd.isna(x) or max_trips == 0:\n",
    "        return 3\n",
    "    return 3 + 10 * (x / max_trips)\n",
    "\n",
    "for _, r in spatial.dropna(subset=[\"latitude\", \"longitude\"]).iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[r[\"latitude\"], r[\"longitude\"]],\n",
    "        radius=radius(r[\"origin_trips\"]),\n",
    "        popup=f\"{r.get('name','')} | trips={int(r['origin_trips'])} | q={r['usage_quartile']}\",\n",
    "        fill=True,\n",
    "    ).add_to(m)\n",
    "\n",
    "m\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Task 3.3 — Pattern discovery (choose ONE insight plot)\n",
    "# Example: compare top 10 busiest vs 10 quietest origin stations.\n",
    "\n",
    "# %%\n",
    "top10 = spatial.sort_values(\"origin_trips\", ascending=False).head(10)\n",
    "bottom10 = spatial.sort_values(\"origin_trips\", ascending=True).head(10)\n",
    "\n",
    "compare = pd.concat([\n",
    "    top10.assign(group=\"Top 10\"),\n",
    "    bottom10.assign(group=\"Bottom 10\"),\n",
    "])\n",
    "\n",
    "# Simple bar plot\n",
    "plt.figure()\n",
    "# keep readable order\n",
    "compare = compare.sort_values([\"group\", \"origin_trips\"], ascending=[True, False])\n",
    "plt.barh(compare[\"group\"] + \" - \" + compare[\"start_station_id\"].astype(str), compare[\"origin_trips\"])\n",
    "plt.title(\"Top 10 vs Bottom 10 stations by trip origins\")\n",
    "plt.xlabel(\"Trips originated (count)\")\n",
    "plt.ylabel(\"Station\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# **Caption (required, 2–3 sentences):**\n",
    "# Replace this caption with your interpretation:\n",
    "# - What does the plot show?\n",
    "# - Why is it interesting / what hypothesis does it suggest?\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Preliminary hypotheses (200–300 words)\n",
    "# - Temporal hypothesis\n",
    "# - Spatial hypothesis\n",
    "# - Surprise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
